{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96ee7767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "493a83ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence:\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "912f5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "#     s = unicodeToAscii(s.lower().strip())\n",
    "#     s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "#     s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70106558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0a1d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e881e195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 6000 segments\n",
      "Counting words...\n",
      "Counted words:\n",
      "767\n"
     ]
    }
   ],
   "source": [
    "def prepareData(data_path = 'FraudedRawData'):\n",
    "    train_segments = []\n",
    "    val_segments = []\n",
    "    test_segments = []\n",
    "\n",
    "    print(\"Reading lines...\")\n",
    "    # Read the file and split into lines\n",
    "    for i in range(40):\n",
    "        fname = f'{data_path}/User{i}'\n",
    "        file1 = open(fname, 'r')\n",
    "        Lines = file1.readlines()\n",
    "        striped_lines = []\n",
    "        for ind, line in enumerate(Lines):\n",
    "            striped_lines += [line.rstrip('\\n') ]\n",
    "            if (ind+1)%100 == 0: #segment is 100 commands                \n",
    "                if ind - 5000 < 0: #train data\n",
    "                    train_segments.append(striped_lines)\n",
    "                else: #validation/test data\n",
    "                    if i<10: #validation\n",
    "                        val_segments.append(striped_lines)\n",
    "                    else: #test \n",
    "                        test_segments.append(striped_lines)\n",
    "                striped_lines = []\n",
    "                \n",
    "    # Split every segment into pairs\n",
    "#      = [[s for s in l.split('\\t')] for l in lines]\n",
    "    segment_lang = Lang()\n",
    "    all_segments = train_segments+val_segments+test_segments\n",
    "    print(\"Read %s segments\" % len(all_segments))\n",
    "    print(\"Counting words...\")\n",
    "    for seg in all_segments:\n",
    "        segment_lang.addSentence(seg)\n",
    "    print(\"Counted words:\")\n",
    "    print(segment_lang.n_words)\n",
    "    return segment_lang, train_segments, val_segments, test_segments#pairs\n",
    "\n",
    "\n",
    "lang, train_segments, val_segments, test_segments = prepareData()\n",
    "# print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbf5fe10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1000, 3000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_segments), len(val_segments), len(test_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "070d09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9484467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, segment):\n",
    "    return [lang.word2index[word] for word in segment]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, segment):\n",
    "    indexes = indexesFromSentence(lang, segment)\n",
    "#     indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def pairTensorsFromSentence(segment):\n",
    "    #In our case of encoder-decoder the target output is the same as the input\n",
    "    input_tensor = tensorFromSentence(lang, segment)\n",
    "    target_tensor = tensorFromSentence(lang, segment)\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7b593ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairTensorsFromSentence(segments[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c6e17",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d0abf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. \n",
    "For every input word the encoder outputs a vector and a hidden state, \n",
    "and uses the hidden state for the next input word.\n",
    "\"\"\"\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5201e34",
   "metadata": {},
   "source": [
    "### Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40d00646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIf only the context vector is passed between the encoder and decoder, \\nthat single vector carries the burden of encoding the entire sentence.\\n\\nAttention allows the decoder network to “focus” on a different part of the encoder’s outputs for every step \\nof the decoder’s own outputs. First we calculate a set of attention weights. \\nThese will be multiplied by the encoder output vectors to create a weighted combination. \\nThe result (called attn_applied in the code) should contain information about that specific part of the input sequence, \\nand thus help the decoder choose the right output words.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "If only the context vector is passed between the encoder and decoder, \n",
    "that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to “focus” on a different part of the encoder’s outputs for every step \n",
    "of the decoder’s own outputs. First we calculate a set of attention weights. \n",
    "These will be multiplied by the encoder output vectors to create a weighted combination. \n",
    "The result (called attn_applied in the code) should contain information about that specific part of the input sequence, \n",
    "and thus help the decoder choose the right output words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc5bf0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, seg_length=segment_len):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, seg_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out1 = nn.Linear(self.hidden_size, self.hidden_size//2)\n",
    "\n",
    "        self.out2 = nn.Linear(self.hidden_size//2, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out1(output[0])\n",
    "        output = F.log_softmax(self.out2(output), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9308d0",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5cefa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To train we run the input sentence through the encoder, \n",
    "and keep track of every output and the latest hidden state. \n",
    "Then the decoder is given the <SOS> token as its first input, \n",
    "and the last hidden state of the encoder as its first hidden state.\n",
    "\"\"\"\n",
    "\n",
    "teacher_forcing_ratio = 1#0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, seg_length=segment_len):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(seg_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "#             if decoder_input.item() == EOS_token:\n",
    "#                 break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88681a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a helper function to print time elapsed and estimated time remaining given the current time and progress %.\n",
    "\"\"\"\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1e7e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The whole training process looks like this:\n",
    "\n",
    "* Start a timer\n",
    "\n",
    "* Initialize optimizers and criterion\n",
    "\n",
    "* Create set of training pairs\n",
    "\n",
    "* Start empty losses array for plotting\n",
    "\n",
    "Then we call train many times and occasionally print the progress (% of examples, time so far, estimated time) and average loss.\n",
    "\"\"\"\n",
    "def trainIters(encoder, decoder, segments, epochs=50, print_every=500, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [pairTensorsFromSentence(seg)\n",
    "                      for seg in segments]\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    n_iters = len(segments)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f'epoch{epoch}/{epochs}:')\n",
    "        for iter in range(1, n_iters + 1):\n",
    "            training_pair = training_pairs[iter - 1]\n",
    "            input_tensor = training_pair[0]\n",
    "            target_tensor = training_pair[1]\n",
    "\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if iter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                             iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "            if iter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa0c1ba",
   "metadata": {},
   "source": [
    "### Plotting results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42072dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbbd02d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "encoder1 = EncoderRNN(lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, lang.n_words, dropout_p=0).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca808653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill model with the trained weights\n",
    "model_path = \"attn_decoder1 - 0.2028, 0.1772, 0.1773, 0.2168.pth\"\n",
    "attn_decoder1.load_state_dict(torch.load(model_path,  map_location=torch.device('cpu')))\n",
    "# attn_decoder1 = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ba57aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill model with the trained weights\n",
    "model_path = \"encoder1 - 0.2028, 0.1772, 0.1773, 0.2168.pth\"\n",
    "encoder1.load_state_dict(torch.load(model_path,  map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6fe57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainIters(encoder1, attn_decoder1, segments=train_segments, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c435b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(encoder1.state_dict(), f\"encoder1 - night 64 do1.pth\")\n",
    "# torch.save(attn_decoder1.state_dict(), f\"attn_decoder1 - night 64 do1.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143dfdf7",
   "metadata": {},
   "source": [
    "### Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c64ac40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation is mostly the same as training, \n",
    "but there are no targets so we simply feed the decoder’s predictions back to itself for each step. \n",
    "Every time it predicts a word we add it to the output string, and if it predicts the EOS token we stop there. \n",
    "We also store the decoder’s attention outputs for display later.\n",
    "\"\"\"\n",
    "def evaluate(encoder, decoder, sentence, seg_length=segment_len):\n",
    "    mistakes = 0\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(seg_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(seg_length, seg_length)\n",
    "\n",
    "        for di in range(seg_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            desired_i = input_tensor[di]\n",
    "            top10 = decoder_output.data.topk(50)[1].view(-1).numpy()\n",
    "            cur_mistake = 100\n",
    "            for index, i in enumerate(top10):\n",
    "                if desired_i == i: cur_mistake=index\n",
    "#             print(di, cur_mistake, mistakes)\n",
    "            mistakes+=cur_mistake\n",
    "#             print(decoder_output.data, topv, topi)\n",
    "#             break\n",
    "#             if topi.item() == EOS_token:\n",
    "#                 decoded_words.append('<EOS>')\n",
    "#                 break\n",
    "#             else:\n",
    "            decoded_words.append(lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1], mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a982afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "challengeToFill = pd.read_csv('challengeToFill.csv', index_col=False)\n",
    "challengeToFillFilledSeq2Seqmodel = challengeToFill.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c769e",
   "metadata": {},
   "source": [
    "# Evaluate on validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29e59c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iter iver validation set csv:\n",
    "y_val = []\n",
    "for user_ind in range(0,10):\n",
    "    for segment_ind in range(51,151):\n",
    "        label = challengeToFillFilledSeq2Seqmodel.iloc[user_ind,segment_ind]\n",
    "#         print(user_ind, segment_ind-1, label)\n",
    "        y_val.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d764282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [02:01<00:00,  8.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "for ind, seg in tqdm(enumerate(val_segments), total= len(val_segments)):\n",
    "    output_words, attentions, mistakes = evaluate(encoder1, attn_decoder1, seg)\n",
    "    alert = False\n",
    "#     print(ind, mistakes)\n",
    "    if mistakes>2500: \n",
    "#         print('gatcha--------------------------')\n",
    "#         print(ind, mistakes)\n",
    "        alert = True\n",
    "    if y_val[ind] == 0:#legitimate\n",
    "        if alert: FP+=1\n",
    "        else: TN+=1\n",
    "    else:\n",
    "        if alert: TP += 1\n",
    "        else: FN += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5b0ff5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:95, TN:110, FP:790, FN:5\n",
      "965\n"
     ]
    }
   ],
   "source": [
    "#mistakes>500: \n",
    "print(f'TP:{TP}, TN:{TN}, FP:{FP}, FN:{FN}')\n",
    "grade = 9*TP + TN\n",
    "print(grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "158ee4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:90, TN:189, FP:711, FN:10\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "#mistakes>1500: \n",
    "print(f'TP:{TP}, TN:{TN}, FP:{FP}, FN:{FN}')\n",
    "grade = 9*TP + TN\n",
    "print(grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e51574a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:74, TN:268, FP:632, FN:26\n",
      "934\n"
     ]
    }
   ],
   "source": [
    "#mistakes>2500: \n",
    "print(f'TP:{TP}, TN:{TN}, FP:{FP}, FN:{FN}')\n",
    "grade = 9*TP + TN\n",
    "print(grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ed2d01",
   "metadata": {},
   "source": [
    "# Evaluate on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cabfa4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3000/3000 [05:58<00:00,  8.37it/s]\n"
     ]
    }
   ],
   "source": [
    "y_preds = []\n",
    "for ind, seg in tqdm(enumerate(test_segments), total= len(test_segments)):\n",
    "    output_words, attentions, mistakes = evaluate(encoder1, attn_decoder1, seg)\n",
    "    alert = 0\n",
    "#     print(ind, mistakes)\n",
    "    if mistakes>500: \n",
    "#         print('gatcha--------------------------')\n",
    "#         print(ind, mistakes)\n",
    "        alert = 1\n",
    "    y_preds.append(alert)\n",
    "\n",
    "#iter over test set csv:\n",
    "preds_counter = 0\n",
    "for user_ind in range(10,40):\n",
    "    for segment_ind in range(51,151):\n",
    "        challengeToFillFilledSeq2Seqmodel.iloc[user_ind,segment_ind] = y_preds[preds_counter]\n",
    "        preds_counter+=1\n",
    "#         print(user_ind, segment_ind-1, label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b6136bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challengeToFillFilledSeq2Seqmodel.to_csv('challengeToFill_filled_Seq2Seq_all_users500mistakes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8093b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e811776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "530aa637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3000/3000 [05:22<00:00,  9.30it/s]\n"
     ]
    }
   ],
   "source": [
    "y_preds = []\n",
    "for ind, seg in tqdm(enumerate(test_segments), total= len(test_segments)):\n",
    "    output_words, attentions, mistakes = evaluate(encoder1, attn_decoder1, seg)\n",
    "    alert = 0\n",
    "#     print(ind, mistakes)\n",
    "    if mistakes>2500: \n",
    "#         print('gatcha--------------------------')\n",
    "#         print(ind, mistakes)\n",
    "        alert = 1\n",
    "    y_preds.append(alert)\n",
    "\n",
    "#iter over test set csv:\n",
    "preds_counter = 0\n",
    "for user_ind in range(10,40):\n",
    "    for segment_ind in range(51,151):\n",
    "        challengeToFillFilledSeq2Seqmodel.iloc[user_ind,segment_ind] = y_preds[preds_counter]\n",
    "        preds_counter+=1\n",
    "#         print(user_ind, segment_ind-1, label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "107d8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "challengeToFillFilledSeq2Seqmodel.to_csv('challengeToFill_filled_Seq2Seq_all_users2500mistakes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce0fe8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4dd8d958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3000/3000 [05:39<00:00,  8.84it/s]\n"
     ]
    }
   ],
   "source": [
    "y_preds = []\n",
    "for ind, seg in tqdm(enumerate(test_segments), total= len(test_segments)):\n",
    "    output_words, attentions, mistakes = evaluate(encoder1, attn_decoder1, seg)\n",
    "    alert = 0\n",
    "#     print(ind, mistakes)\n",
    "    if mistakes>3500: \n",
    "#         print('gatcha--------------------------')\n",
    "#         print(ind, mistakes)\n",
    "        alert = 1\n",
    "    y_preds.append(alert)\n",
    "\n",
    "#iter over test set csv:\n",
    "preds_counter = 0\n",
    "for user_ind in range(10,40):\n",
    "    for segment_ind in range(51,151):\n",
    "        challengeToFillFilledSeq2Seqmodel.iloc[user_ind,segment_ind] = y_preds[preds_counter]\n",
    "        preds_counter+=1\n",
    "#         print(user_ind, segment_ind-1, label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e69a76eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challengeToFillFilledSeq2Seqmodel.to_csv('challengeToFill_filled_Seq2Seq_all_users3500mistakes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f36262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
